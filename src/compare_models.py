#!/usr/bin/env python3
"""
Model Comparison Script

Compares all models generated by run.py
Automatically finds all available results
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import sys
from pathlib import Path


def load_results(filepath):
    """Load results from JSON file."""
    try:
        with open(filepath, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return None


def find_all_results():
    """Automatically find all result files in the project directory."""
    result_files = {}
    
    # Define all possible result locations
    search_paths = {
        # Original models (if they exist)
        'Centralized Original': 'centralized_patchcore_results.json',
        'Federated Original': 'global_final_results.json',
        
        # New improved models from overnight training
        'Centralized Improved': 'results/improved/centralized_improved_results.json',
        'Aggregated Category-Aware': 'results/aggregated/federated_category_aware_results.json',
        'Federated FedAvg': 'results/federated/fedavg/global_final_improved_results.json',
        'Federated FedProx': 'results/federated/fedprox/global_final_improved_results.json',
        'Federated Fairness-Aware': 'results/federated/fairness/global_final_improved_results.json',
        'Federated Category-Aware': 'results/federated/category_aware/global_final_improved_results.json',
    }
    
    # Check which files exist
    for name, path in search_paths.items():
        if os.path.exists(path):
            result_files[name] = path
    
    return result_files


def compare_models():
    """Compare all available models and generate visualizations."""
    
    print("\n" + "=" * 70)
    print("MODEL COMPARISON REPORT")
    print("=" * 70 + "\n")
    
    # Find all available models
    models = find_all_results()
    
    if not models:
        print("\nError: No results found!")
        print("Make sure you've run training and evaluation first.")
        return
    
    print(f"Found {len(models)} models to compare:\n")
    
    # Load all available results
    all_results = {}
    for name, filepath in models.items():
        results = load_results(filepath)
        if results:
            all_results[name] = results
            print(f"Loaded: {name:<35} ({filepath})")
        else:
            print(f"Failed: {name}")
    
    if not all_results:
        print("\nError: No valid results found!")
        return
    
    print(f"\n{'=' * 80}")
    print("GLOBAL METRICS COMPARISON")
    print(f"{'=' * 80}")
    
    # Build comparison table
    metrics_data = []
    for model_name, results in all_results.items():
        metrics_data.append({
            'Model': model_name,
            'Image AUROC': results.get('image_auroc', 0),
            'Image AP': results.get('image_ap', 0),
            'Pixel AUROC': results.get('pixel_auroc', 0),
            'TPR@95%': results.get('tpr_at_tnr_95', 0) * 100,
            'AUROC Range': results.get('auroc_range', 0),
            'Disparate Impact': results.get('disparate_impact', 0),
        })
    
    df = pd.DataFrame(metrics_data)
    
    # Sort by Image AUROC descending
    df = df.sort_values('Image AUROC', ascending=False)
    
    print("\n" + df.to_string(index=False))
    
    # Save CSV
    df.to_csv('model_comparison.csv', index=False)
    print(f"\nSaved: model_comparison.csv")
    
    # Per-category comparison
    print(f"\n{'=' * 80}")
    print("PER-CATEGORY AUROC COMPARISON")
    print(f"{'=' * 80}")
    
    # Collect all categories
    all_categories = set()
    for results in all_results.values():
        for cat in results.get('per_category', []):
            all_categories.add(cat['category'])
    
    # Build comparison table
    cat_comparison = []
    for category in sorted(all_categories):
        row = {'Category': category}
        for model_name, results in all_results.items():
            auroc = None
            for cat in results.get('per_category', []):
                if cat['category'] == category:
                    auroc = cat.get('auroc')
                    break
            row[model_name] = auroc if auroc else 0
        cat_comparison.append(row)
    
    df_cat = pd.DataFrame(cat_comparison)
    print("\n" + df_cat.to_string(index=False))
    
    # Save per-category CSV
    df_cat.to_csv('model_comparison_per_category.csv', index=False)
    print(f"\nSaved: model_comparison_per_category.csv")
    
    # Calculate improvements if we have both original and improved
    if 'Centralized Original' in all_results and 'Centralized Improved' in all_results:
        print(f"\n{'=' * 80}")
        print("IMPROVEMENTS (Improved vs Original)")
        print(f"{'=' * 80}")
        
        orig = all_results['Centralized Original']
        impr = all_results['Centralized Improved']
        
        improvements = {
            'Image AUROC': (impr.get('image_auroc', 0) - orig.get('image_auroc', 0)) * 100,
            'TPR@95%': (impr.get('tpr_at_tnr_95', 0) - orig.get('tpr_at_tnr_95', 0)) * 100,
            'AUROC Range': (orig.get('auroc_range', 1) - impr.get('auroc_range', 1)) * 100,
            'Disparate Impact': (impr.get('disparate_impact', 0) - orig.get('disparate_impact', 0)) * 100,
        }
        
        print(f"\n{'Metric':<25} {'Change':<15} {'Status'}")
        print("-" * 60)
        for metric, change in improvements.items():
            if 'Range' in metric:  # Lower is better
                status = "Better" if change > 0 else "Worse"
                symbol = "↓"
            else:  # Higher is better
                status = "Better" if change > 0 else "Worse"
                symbol = "↑"
            
            if metric == 'Image AUROC':
                print(f"{metric:<25} {symbol}{abs(change):.2f}%{'':<8} {status}")
            elif metric == 'TPR@95%':
                print(f"{metric:<25} {symbol}{abs(change):.1f}pp{'':<7} {status}")
            else:
                print(f"{metric:<25} {symbol}{abs(change):.2f}%{'':<8} {status}")
    
    # Ranking
    print(f"\n{'=' * 80}")
    print("MODEL RANKING")
    print(f"{'=' * 80}")
    
    # Rank by different metrics
    print("\nBy Image AUROC:")
    ranked = sorted(all_results.items(), key=lambda x: x[1].get('image_auroc', 0), reverse=True)
    for i, (name, res) in enumerate(ranked, 1):
        auroc = res.get('image_auroc', 0)
        print(f"  {i}. {name:<35} {auroc:.4f}")
    
    print("\nBy Fairness (Disparate Impact):")
    ranked = sorted(all_results.items(), key=lambda x: x[1].get('disparate_impact', 0), reverse=True)
    for i, (name, res) in enumerate(ranked, 1):
        di = res.get('disparate_impact', 0)
        if di >= 0.8:
            fairness = "Excellent"
        elif di >= 0.7:
            fairness = "Good"
        elif di >= 0.6:
            fairness = "Fair"
        else:
            fairness = "Poor"
        print(f"  {i}. {name:<35} {di:.4f} {fairness}")
    
    print("\nBy AUROC Range (lower is better):")
    ranked = sorted(all_results.items(), key=lambda x: x[1].get('auroc_range', 1))
    for i, (name, res) in enumerate(ranked, 1):
        range_val = res.get('auroc_range', 0)
        print(f"  {i}. {name:<35} {range_val:.4f}")
    
    # Visualization
    print(f"\n{'=' * 80}")
    print("GENERATING VISUALIZATIONS")
    print(f"{'=' * 80}\n")
    
    try:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # Plot 1: Global metrics comparison
        ax = axes[0, 0]
        metrics_to_plot = ['Image AUROC', 'Pixel AUROC', 'Disparate Impact']
        x = np.arange(len(all_results))
        width = 0.25
        
        for i, metric in enumerate(metrics_to_plot):
            values = [df[df['Model'] == model][metric].values[0] 
                     for model in all_results.keys()]
            ax.bar(x + i*width, values, width, label=metric, alpha=0.8)
        
        ax.set_xlabel('Model', fontsize=10)
        ax.set_ylabel('Score', fontsize=10)
        ax.set_title('Performance Metrics Comparison', fontsize=12, fontweight='bold')
        ax.set_xticks(x + width)
        ax.set_xticklabels(all_results.keys(), rotation=45, ha='right', fontsize=8)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        ax.set_ylim([0, 1])
        
        # Plot 2: Fairness metrics
        ax = axes[0, 1]
        fairness_models = list(all_results.keys())
        di_values = [all_results[m].get('disparate_impact', 0) for m in fairness_models]
        range_values = [all_results[m].get('auroc_range', 0) for m in fairness_models]
        
        x = np.arange(len(fairness_models))
        ax.bar(x - 0.2, di_values, 0.4, label='Disparate Impact (higher better)', alpha=0.8, color='green')
        ax.bar(x + 0.2, range_values, 0.4, label='AUROC Range (lower better)', alpha=0.8, color='orange')
        
        ax.set_xlabel('Model', fontsize=10)
        ax.set_ylabel('Score', fontsize=10)
        ax.set_title('Fairness Metrics (DI higher better, Range lower better)', fontsize=12, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(fairness_models, rotation=45, ha='right', fontsize=8)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        ax.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='DI Good threshold')
        ax.axhline(y=0.35, color='orange', linestyle='--', alpha=0.5, label='Range Good threshold')
        
        # Plot 3: Per-category heatmap
        ax = axes[1, 0]
        heatmap_data = df_cat.set_index('Category')
        
        # Only plot if we have reasonable number of categories
        if len(heatmap_data) <= 15:
            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', 
                       vmin=0.4, vmax=0.9, ax=ax, cbar_kws={'label': 'AUROC'})
            ax.set_title('Per-Category AUROC Heatmap', fontsize=12, fontweight='bold')
            plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=8)
        else:
            ax.text(0.5, 0.5, f'Too many categories ({len(heatmap_data)}) for heatmap', 
                   ha='center', va='center', transform=ax.transAxes)
        
        # Plot 4: TPR@95% comparison
        ax = axes[1, 1]
        tpr_values = [df[df['Model'] == model]['TPR@95%'].values[0] 
                     for model in all_results.keys()]
        colors = ['green' if x > 15 else 'orange' if x > 10 else 'red' for x in tpr_values]
        ax.barh(list(all_results.keys()), tpr_values, color=colors, alpha=0.7)
        ax.set_xlabel('TPR @ 95% TNR (%)', fontsize=10)
        ax.set_title('Detection Rate at 95% Specificity', fontsize=12, fontweight='bold')
        ax.axvline(x=15, color='green', linestyle='--', alpha=0.5, label='Good (>15%)')
        ax.axvline(x=10, color='orange', linestyle='--', alpha=0.5, label='Fair (>10%)')
        ax.legend()
        ax.grid(axis='x', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
        print("Saved: model_comparison.png")
        
    except Exception as e:
        print(f"Warning: Could not generate plots: {e}")
    
    print(f"\n{'=' * 80}")
    print("COMPARISON COMPLETE")
    print(f"{'=' * 80}")
    print("\nOutput files:")
    print("  - model_comparison.csv")
    print("  - model_comparison_per_category.csv")
    print("  - model_comparison.png")
    print()


if __name__ == "__main__":
    compare_models()